{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6ddd48-b44f-43c3-8069-86bfb0304fc8",
   "metadata": {},
   "source": [
    "# Lectures 1 - 2\n",
    "\n",
    "[link](https://www.youtube.com/watch?v=mbyG85GZ0PI&list=PLnIDYuXHkit4LcWjDe0EwlE57WiGlBs08&index=1)\n",
    "\n",
    "Basic principle: *using a set of observations to uncover the behaviour of an underlying process*. In statistics the underlying process is a probability distribution, and inputs are samples.\n",
    "\n",
    "The essence of machine learning:\n",
    "\n",
    "* A pattern exists\n",
    "* We cannot write an analytical expression f(x) $\\rightarrow$ hence learning from data\n",
    "* We have data\n",
    "\n",
    "## Components of learning\n",
    "\n",
    "How do you know that a specific variable is related to the outcomes? Need to find a correlation between variables and outcomes first (e.g. dot product).\n",
    "\n",
    "E.g. lending\n",
    "\n",
    "1. Input: x (customer application(\n",
    "2. Output: y (good/bad customer)\n",
    "3. Target function: $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ (ideal credit approval formula), **UNKNOWN**\n",
    "4. Data: $(x_1,y_1), (x_2,y_2),\\dots,(x_N,y_N)$\n",
    "   \n",
    "$\\qquad \\big\\downarrow \\qquad \\big\\downarrow \\qquad \\big\\downarrow$\n",
    "\n",
    "* Hypotesis: $g: \\mathcal{X} \\rightarrow \\mathcal{Y}$ (the function we create)\n",
    "\n",
    "They hypotesis set $\\{\\mathcal{X}\\}, g \\in{\\mathcal{H}}$ + learning algorithm = *learning model*\n",
    "\n",
    "The learning algorithm is the process that at each steps achieves a better approximation of the target function e.g. the back propagation for neural network.\n",
    "\n",
    "### Example of hypotesis set\n",
    "\n",
    "An example of hypotesis set is the *perceptron*.\n",
    "\n",
    "* input: $\\textbf{x}=(x_1,\\dots, x_d)$\n",
    "* hypotesis set: $h(\\textbf{x})=sign(\\sum_{i=1}^{d}{w_ix_i} - threshold)$ \n",
    "\n",
    "$g$ is the function in the hypotesis set that best approximates $f$, i.e. the specific value of $w$ weights and threshold that minimises the error $|f-h|$.\n",
    "\n",
    "$-threshold = w_0, x_0=1 \\rightarrow h(\\textbf{x})=sign(\\sum_{i=0}^{d}{w_ix_i})$\n",
    "\n",
    "The hypotesis set tells you the resources you are dealing with, in this case the weighted linear combination of input data and threshold.\n",
    "\n",
    "### Example of learning model\n",
    "\n",
    "Given a number of inputs and outputs $((x_1,y_1),\\dots,(x_N,y_N)$ and a perceptron hypotesis set, the algorithm at each iteration:\n",
    "1. finds a misclassified point such as $\\mathbf{w}^T\\mathbf{x_n} \\ne y_n$\n",
    "2. updates the weights: $\\mathbf{w} = \\mathbf{w} + y_nx_n, \\  where\\  y \\in \\{-1,1\\}$\n",
    "\n",
    "If the output is positive and the perceptron returns a negative value add a positive quantity to the weight otherwise add a negative quantity to move it in the opposite direction.\n",
    "\n",
    "In the case of the perceptron classifier learning is guaranteed if data is linearly separable.\n",
    "\n",
    "Learning algorithm picks $g$ from $\\mathcal{H}$ to approximate $f$.\n",
    "\n",
    "# Is learning feasible?\n",
    "\n",
    "Can we say something when data is outside of the sample data we have?\n",
    "\n",
    "### Hoeffding inequality\n",
    "\n",
    "$\\mathbb{P}[|\\nu - \\mu| \\gt \\epsilon] \\le 2e^{-2\\epsilon^2N}$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mu = E_{out}(h)$ out of sample (true) expected (average) value\n",
    "* $\\nu = E_{in}(h)$ in sample (estimated) expected (average) value\n",
    "\n",
    "$E_{in}\\ and\\ E_{out}$ are dependent on the hypotesis $h$.\n",
    "\n",
    "$E$ is the expected value of a random variable e.g. the probability of drawing a red marble out of a jar containing\n",
    "red and green marbles: the $E_{out}$ value is the percentage of read marble in the jar, $E_{in}$ is the percentage of red marble in a sample of $N$ marbles drawn out of the jar.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
